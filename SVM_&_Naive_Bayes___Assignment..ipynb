{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?"
      ],
      "metadata": {
        "id": "vxEdDfLxQT4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        " A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression.\n",
        "SVM works by finding the optimal hyperplane that separates data points of different classes with the maximum margin.\n",
        "\n",
        "\n",
        "The points closest to the hyperplane are called support vectors. These points are critical in defining the decision boundary.\n",
        "\n",
        "\n",
        "For non-linear data, SVM uses the kernel trick to map data into higher dimensions, where a linear separation is possible.\n",
        "\n",
        "\n",
        "Thus, SVM is effective for high-dimensional spaces and provides robust performance in classification problems.\n"
      ],
      "metadata": {
        "id": "ukbGPiTMQWzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM."
      ],
      "metadata": {
        "id": "uBr7XNRGQZe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Hard Margin SVM:\n",
        "\n",
        "\n",
        "Assumes the data is perfectly linearly separable.\n",
        "\n",
        "\n",
        "No misclassification is allowed.\n",
        "\n",
        "\n",
        "Can lead to overfitting if data is noisy.\n",
        "\n",
        "\n",
        "Soft Margin SVM:\n",
        "\n",
        "\n",
        "Allows some misclassifications (controlled by penalty parameter C).\n",
        "\n",
        "\n",
        "Strikes a balance between maximizing the margin and minimizing misclassification error.\n",
        "\n",
        "\n",
        "Works better in real-world noisy datasets.\n",
        "\n",
        "\n",
        "Key Difference: Hard margin enforces strict separation, while soft margin introduces flexibility to improve generalization.\n"
      ],
      "metadata": {
        "id": "DFs5jROXQciF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n"
      ],
      "metadata": {
        "id": "3BUZAKFxQe7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        " The Kernel Trick allows SVM to perform classification in higher-dimensional spaces without explicitly transforming the data. Instead, it uses a kernel function to compute similarity between points in that space.\n",
        "Example: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "\n",
        "Formula:\n",
        "K(x,y)=exp(−γ∥x−y∥2)\n",
        "Use Case: Useful when data is not linearly separable in the original space, such as in image classification or complex pattern recognition tasks.\n"
      ],
      "metadata": {
        "id": "DlfalomxQg4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?"
      ],
      "metadata": {
        "id": "HeTXSBANQisu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        " The Naïve Bayes Classifier is a probabilistic classifier based on Bayes’ Theorem. It assumes that all features are independent given the class label.\n",
        "It calculates the probability of each class for a given instance and selects the class with the highest probability.\n",
        "\n",
        "\n",
        "It is called “naïve” because the assumption of feature independence is rarely true in real-world data. Despite this, it works well in practice, especially for text classification tasks like spam detection.\n"
      ],
      "metadata": {
        "id": "uMmY42Y_QkbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n"
      ],
      "metadata": {
        "id": "zxw51dNnQmY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Gaussian Naïve Bayes:\n",
        "\n",
        "\n",
        "Assumes features follow a normal (Gaussian) distribution.\n",
        "\n",
        "\n",
        "Used for continuous data (e.g., Iris dataset with petal/sepal lengths).\n",
        "\n",
        "\n",
        "Multinomial Naïve Bayes:\n",
        "\n",
        "\n",
        "Works with count-based features.\n",
        "\n",
        "\n",
        "Used for text classification (e.g., word counts in spam detection).\n",
        "\n",
        "\n",
        "Bernoulli Naïve Bayes:\n",
        "\n",
        "\n",
        "Assumes binary features (0/1).\n",
        "\n",
        "\n",
        "Used when features represent presence/absence (e.g., word present or not in email).\n"
      ],
      "metadata": {
        "id": "1nDOJ4-mQoPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to: ● Load the Iris dataset ● Train an SVM Classifier with a linear kernel ● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "jUeT55rsQqim"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U11n1KkGPVsR",
        "outputId": "0b9280ce-f614-4b3d-8d7e-9977ec9bc1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors: [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with linear kernel\n",
        "svm_linear = SVC(kernel=\"linear\", random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Support Vectors:\", svm_linear.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to: ● Load the Breast Cancer dataset ● Train a Gaussian Naïve Bayes model ● Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "BDnPk8U3Q5CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gnb.predict(X)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foPAZKNSQ_q5",
        "outputId": "be5339e2-a917-4782-cb46-45bcaea63eb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.95      0.89      0.92       212\n",
            "      benign       0.94      0.97      0.95       357\n",
            "\n",
            "    accuracy                           0.94       569\n",
            "   macro avg       0.94      0.93      0.94       569\n",
            "weighted avg       0.94      0.94      0.94       569\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to: ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. ● Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "atmeEloqRCxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'kernel': ['rbf']}\n",
        "\n",
        "# GridSearch\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsnOra1YRDf5",
        "outputId": "0cf3cbf6-e44d-41ca-c2f0-bada5360bffc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best Accuracy: 0.6946666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to: ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). ● Print the model's ROC-AUC score for its predictions.\n"
      ],
      "metadata": {
        "id": "1prlqIv9RGnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_20newsgroups(subset='train', categories=['sci.space', 'rec.autos'], remove=('headers','footers','quotes'))\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb.predict_proba(X_test)\n",
        "\n",
        "# ROC-AUC\n",
        "y_test_bin = label_binarize(y_test, classes=[0,1])\n",
        "roc_auc = roc_auc_score(y_test_bin, y_pred[:,1])\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oK7DlubRIjK",
        "outputId": "720da538-df73-4ed4-955a-6a123b5bd794"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9777370185314023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain: ● Text with diverse vocabulary ● Potential class imbalance (far more legitimate emails than spam) ● Some incomplete or missing data Explain the approach you would take to: ● Preprocess the data (e.g. text vectorization, handling missing data) ● Choose and justify an appropriate model (SVM vs. Naïve Bayes) ● Address class imbalance ● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.\n"
      ],
      "metadata": {
        "id": "6U9ZwR1MRPvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach:\n",
        "Preprocessing:\n",
        "\n",
        "\n",
        "Handle missing data (replace with empty string).\n",
        "\n",
        "\n",
        "Convert text into numerical features using TF-IDF vectorization.\n",
        "\n",
        "\n",
        "Model Choice:\n",
        "\n",
        "\n",
        "Naïve Bayes (Multinomial) is preferred for spam detection due to efficiency on text data.\n",
        "\n",
        "\n",
        "SVM could be used if higher accuracy is required but may be slower on large text corpora.\n",
        "\n",
        "\n",
        "Address Class Imbalance:\n",
        "\n",
        "\n",
        "Use SMOTE (oversampling) or class weighting.\n",
        "\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "\n",
        "Precision, Recall, F1-score, ROC-AUC (important due to class imbalance).\n",
        "\n",
        "\n",
        "Business Impact:\n",
        "Automatically filters spam, saving time and improving productivity.\n",
        "\n",
        "\n",
        "Protects against phishing and scams.\n",
        "\n",
        "\n",
        "Enhances customer trust in the company’s communication system.\n"
      ],
      "metadata": {
        "id": "MVag1lmpRZTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10 - Spam Classification (Final Clean Version)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Sample dataset\n",
        "# -----------------------------\n",
        "data = {\n",
        "    \"email\": [\n",
        "        \"Win a free iPhone now\",\n",
        "        \"Meeting tomorrow at 10am\",\n",
        "        \"Congratulations, you won a lottery ticket\",\n",
        "        \"Reminder: Submit project report\",\n",
        "        \"Get cheap loans instantly\",\n",
        "        \"Schedule lunch with client\",\n",
        "        \"Earn money working from home\",\n",
        "        \"Exclusive deal just for you\",\n",
        "        \"Can we reschedule the call?\",\n",
        "        \"URGENT: Your account has been suspended\"\n",
        "    ],\n",
        "    \"label\": [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]  # 1 = Spam, 0 = Not Spam\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Preprocessing\n",
        "# -----------------------------\n",
        "df['email'] = df['email'].fillna(\"\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(df['email'])\n",
        "y = df['label']\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Train-Test Split\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Train Naive Bayes Model\n",
        "# -----------------------------\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Evaluation\n",
        "# -----------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Business Impact\n",
        "# -----------------------------\n",
        "print(\"Business Impact:\")\n",
        "print(\"- Saves employees’ time by filtering spam automatically.\")\n",
        "print(\"- Prevents phishing attacks and fraud.\")\n",
        "print(\"- Ensures critical emails are delivered properly.\")\n",
        "print(\"- Improves organizational productivity and security.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSrN250GRYrE",
        "outputId": "bd7bb2af-046a-49c9-bb8c-3fa1dacb758a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.67      1.00      0.80         2\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.33      0.50      0.40         3\n",
            "weighted avg       0.44      0.67      0.53         3\n",
            "\n",
            "Confusion Matrix:\n",
            " [[0 1]\n",
            " [0 2]]\n",
            "Business Impact:\n",
            "- Saves employees’ time by filtering spam automatically.\n",
            "- Prevents phishing attacks and fraud.\n",
            "- Ensures critical emails are delivered properly.\n",
            "- Improves organizational productivity and security.\n"
          ]
        }
      ]
    }
  ]
}